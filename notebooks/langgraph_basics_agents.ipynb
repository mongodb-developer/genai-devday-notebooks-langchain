{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph Basics | (RAG + Multi-Agent)\n",
    "\n",
    "Over the course of this notebook, we will build an agentic RAG application using LangGraph with increasing complexity. We will start with a simple RAG flow, and then add conditional branching, memory, human in the loop, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HIL](./images/human_in_the_loop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-work: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set your environment variables locally in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pymongo'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpymongo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MongoClient\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Add parent directory to path to import from utils\u001b[39;00m\n\u001b[32m      6\u001b[39m sys.path.append(os.path.join(os.path.dirname(os.getcwd())))\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pymongo'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "\n",
    "# Add parent directory to path to import from utils\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd())))\n",
    "from utils import track_progress, set_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using your own MongoDB Atlas cluster, use the connection string for your cluster here\n",
    "MONGODB_URI =  os.environ.get(\"MONGODB_URI\")\n",
    "# Initialize a MongoDB Python client\n",
    "mongodb_client = MongoClient(MONGODB_URI)\n",
    "# Check the connection to the server\n",
    "mongodb_client.admin.command(\"ping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the LLM provider and passkey provided by your workshop instructor\n",
    "# NOTE: LLM_PROVIDER can be set to one of \"aws\"/ \"microsoft\" / \"google\"\n",
    "LLM_PROVIDER = \"aws\"\n",
    "PASSKEY = \"replace-with-passkey\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain API keys from our AI model proxy and set them as environment variables-- DO NOT CHANGE\n",
    "set_env([LLM_PROVIDER,\"voyageai\"], PASSKEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional**: enable LangSmith tracing, get free API key from https://smith.langchain.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"mongodb-genai-devday\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that LangSmith tracing is enabled. If for some reason you can't see traces showing up in LangSmith, this is a great helper command to make sure you can trace!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import utils as langsmith_utils\n",
    "\n",
    "os.environ.get(\"LANGCHAIN_TRACING_V2\")\n",
    "langsmith_utils.tracing_is_enabled()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we're building a RAG application, we're going to create a vector database retriever containing MongoDB documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick an LLM provider of your choice below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.load import load\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from utils import get_llm\n",
    "\n",
    "# Obtain the Langchain LLM object using the `get_llm` function from the `utils`` module.\n",
    "llm = get_llm(LLM_PROVIDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Vector Store MongoDB Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Database name\n",
    "DB_NAME = \"mongodb_genai_devday_agents\"\n",
    "# Name of the collection with full documents- used for summarization\n",
    "FULL_COLLECTION_NAME = \"mongodb_docs\"\n",
    "# Name of the collection for vector search- used for Q&A\n",
    "VS_COLLECTION_NAME = \"mongodb_docs_embeddings\"\n",
    "# Name of the vector search index\n",
    "VS_INDEX_NAME = \"vector_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the `VS_COLLECTION_NAME` collection.\n",
    "vs_collection = mongodb_client[DB_NAME][VS_COLLECTION_NAME]\n",
    "# Connect to the `FULL_COLLECTION_NAME` collection.\n",
    "full_collection = mongodb_client[DB_NAME][FULL_COLLECTION_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert a dataset of MongoDB docs with embeddings into the `VS_COLLECTION_NAME` collection\n",
    "with open(f\"../data/{VS_COLLECTION_NAME}.json\", \"r\") as data_file:\n",
    "    json_data = data_file.read()\n",
    "\n",
    "data = json.loads(json_data)\n",
    "\n",
    "print(f\"Deleting existing documents from the {VS_COLLECTION_NAME} collection.\")\n",
    "vs_collection.delete_many({})\n",
    "vs_collection.insert_many(data)\n",
    "print(\n",
    "    f\"{vs_collection.count_documents({})} documents ingested into the {VS_COLLECTION_NAME} collection.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert a dataset of MongoDB documentation pages into the `FULL_COLLECTION_NAME` collection\n",
    "with open(f\"../data/{FULL_COLLECTION_NAME}.json\", \"r\") as data_file:\n",
    "    json_data = data_file.read()\n",
    "\n",
    "data = json.loads(json_data)\n",
    "\n",
    "print(f\"Deleting existing documents from the {FULL_COLLECTION_NAME} collection.\")\n",
    "full_collection.delete_many({})\n",
    "full_collection.insert_many(data)\n",
    "print(\n",
    "    f\"{full_collection.count_documents({})} documents ingested into the {FULL_COLLECTION_NAME} collection.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_index, check_index_ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector index definition specifying:\n",
    "# path: Path to the embeddings field\n",
    "# numDimensions: Number of embedding dimensions- depends on the embedding model used\n",
    "# similarity: Similarity metric. One of cosine, euclidean, dotProduct.\n",
    "model = {\n",
    "    \"name\": VS_INDEX_NAME,\n",
    "    \"type\": \"vectorSearch\",\n",
    "    \"definition\": {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"type\": \"vector\",\n",
    "                \"path\": \"embedding\",\n",
    "                \"numDimensions\": 384,\n",
    "                \"similarity\": \"cosine\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the `create_index` function from the `utils` module to create a vector search index with the above definition for the `vs_collection` collection\n",
    "create_index(vs_collection, VS_INDEX_NAME, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the `check_index_ready` function from the `utils` module to verify that the index was created and is in READY status before proceeding\n",
    "check_index_ready(vs_collection, VS_INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the `gte-small` model using the Sentence Transformers library\n",
    "embedding_model = SentenceTransformer(\"thenlper/gte-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "# Define a function that takes a piece of text (`text`) as input, embeds it using the `embedding_model` instantiated above and returns the embedding as a list\n",
    "# An array can be converted to a list using the `tolist()` method\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Generate the embedding for a piece of text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text to embed.\n",
    "\n",
    "    Returns:\n",
    "        List[float]: Embedding of the text as a list.\n",
    "    \"\"\"\n",
    "    embedding = embedding_model.encode(text)\n",
    "    return embedding.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "def get_information_for_question_answering(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve information using vector search to answer a user query.\n",
    "\n",
    "    Args:\n",
    "    user_query (str): The user's query string.\n",
    "\n",
    "    Returns:\n",
    "    str: The retrieved information formatted as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate embedding for the `user_query` using the `get_embedding` function defined above\n",
    "    query_embedding = get_embedding(user_query)\n",
    "\n",
    "    # Define an aggregation pipeline consisting of a $vectorSearch stage, followed by a $project stage\n",
    "    # Set the number of candidates to 150 and only return the top 5 documents from the vector search\n",
    "    # In the $project stage, exclude the `_id` field and include only the `body` field and `vectorSearchScore`\n",
    "    # NOTE: Use variables defined previously for the `index`, `queryVector` and `path` fields in the $vectorSearch stage\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"index\": VS_INDEX_NAME,\n",
    "                \"path\": \"embedding\",\n",
    "                \"queryVector\": query_embedding,\n",
    "                \"numCandidates\": 150,\n",
    "                \"limit\": 5,\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"_id\": 0,\n",
    "                \"body\": 1,\n",
    "                \"score\": {\"$meta\": \"vectorSearchScore\"},\n",
    "            }\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Execute the aggregation `pipeline` against the `vs_collection` collection and store the results in `results`\n",
    "    results = vs_collection.aggregate(pipeline)\n",
    "    # create a list of Document objects from the results\n",
    "    docs = [Document(page_content=doc.get(\"body\"), metadata={\"score\": doc.get(\"score\")}) for doc in results]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-work: Background Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to skip this section if you're already familiar with the LangChain ChatModel and Messages concepts.\n",
    "\n",
    "In this course, we'll be using [Chat Models](https://python.langchain.com/v0.2/docs/concepts/#chat-models), which take a sequence of messages as inputs and return chat messages as outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat models in LangChain have a number of [default methods](https://python.langchain.com/v0.2/docs/concepts/#runnable-interface). For now we'll use `invoke`, which call the model on an input.\n",
    "\n",
    "Chat models take [messages](https://python.langchain.com/v0.2/docs/concepts/#messages) as input. LangChain supports various message types, including `HumanMessage`, `AIMessage`, `SystemMessage`, and `ToolMessage`. Let's create a list of messages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "# Some sample messages about orcas\n",
    "messages = [AIMessage(content=f\"So you said you were researching ocean mammals?\", name=\"Model\")]\n",
    "messages.append(HumanMessage(content=f\"Yes, that's right.\",name=\"Marco\"))\n",
    "messages.append(AIMessage(content=f\"Great, what would you like to learn about.\", name=\"Model\"))\n",
    "messages.append(HumanMessage(content=f\"I want to learn about the best place to see Orcas in the US.\", name=\"Marco\"))\n",
    "\n",
    "for m in messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ChatModel interface is consistent across all chat models and models are typically initialized once at the start up each notebooks. The benefit here is that you can easily switch between models without changing the downstream code if you have strong preference for another provider.\n",
    "\n",
    "Let's run our ChatModel on these Messages now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: LangGraph Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Simple RAG](./images/simple_rag.png)\n",
    "\n",
    "We're going to set up a simple RAG workflow while introducing several LangGraph concepts. We're then going to step into LangSmith and see how it can help us while we iterate on our application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now that we've tested out our ChatModel on some Messages let's start learning about some of our Agent primitives. Our first concept is [State](https://langchain-ai.github.io/langgraph/concepts/low_level/#state).\n",
    "\n",
    "State is one of the most important concepts in an Agent. When defining a Graph, you must pass in a schema for State. The State schema serves as the input schema for all Nodes and Edges in the graph. Let's use the `TypedDict` class from python's `typing` module as our schema, which provides type hints for the keys. \n",
    "\n",
    "The State of our RAG application will keep track of the user's question, our RAG app's LLM generated response, and the list of retrieved relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "        question: The user's question\n",
    "        generation: The LLM's generation\n",
    "        documents: List of helpful documents retrieved by the RAG pipeline\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[Document]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Nodes](https://langchain-ai.github.io/langgraph/concepts/low_level/#nodes) are just python functions. As mentioned above, Nodes take in your graph's State as input. \n",
    "\n",
    "The first positional argument is the state, as defined above.\n",
    "\n",
    "Because the state is a `TypedDict` with schema as defined above, each node can access each key in the state, in our case, we could use `state[\"question\"]`.\n",
    "  \n",
    "Nodes return any updates to the state that they want to make. By default, the new value returned by each node will override the prior state value. You can implement custom handling for updates to State using State Reducers, which we will see later in the session.\n",
    "\n",
    "Here, we're going to set up two nodes for our RAG flow:\n",
    "1. retrieve_documents: Retrieves documents from our vector store\n",
    "2. generate_response: Generates an answer from our documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def retrieve_documents(state: State):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = get_information_for_question_answering(question)\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "RAG_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\"\n",
    "\n",
    "def generate_response(state: State):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE RESPONSE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    \n",
    "    # Invoke our LLM with our RAG prompt\n",
    "    rag_prompt_formatted = RAG_PROMPT.format(context=formatted_docs, question=question)\n",
    "    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "    return {\"generation\": generation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Edges](https://langchain-ai.github.io/langgraph/concepts/low_level/#edges) define how your agentic applications progresses from each Node to the next Node.\n",
    "- Normal Edges are used if you want to *always* go from, for example, `node_1` to `node_2`.\n",
    "- [Conditional Edges](https://langchain-ai.github.io/langgraph/reference/graphs/?h=conditional+edge#langgraph.graph.StateGraph.add_conditional_edges) are used want to *optionally* route between nodes.\n",
    " \n",
    "Conditional edges are implemented as functions that return the next node to visit based upon some logic. Note that these functions often use values from our graph's State to determine how to traverse.\n",
    "\n",
    "We'll add some useful conditional edges later, but for now let's take a look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "def conditional_edge_example(state) -> Literal[\"node_1\", \"node_2\"]:\n",
    "    # Often, we will use state to decide on the next node to visit\n",
    "    field_1 = state['field_1'] \n",
    "    field_2 = state['field_2']\n",
    "    if field_1 > field_2:\n",
    "        return \"node_1\"\n",
    "    return \"node_2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We now have defined the schema for our State, written logic for two Nodes, and learned about Edges. Let's stitch those components together to define our simple RAG graph\n",
    "\n",
    "First, we instantiate a graph builder with our State. The [StateGraph class](https://langchain-ai.github.io/langgraph/concepts/low_level/#stategraph) is the graph class that we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we add our two defined nodes to our Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_node(\"retrieve_documents\", retrieve_documents)\n",
    "graph_builder.add_node(\"generate_response\", generate_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define the shape of our graph by adding edges between the nodes.\n",
    "\n",
    "We use the [`START` Node, a special node](https://langchain-ai.github.io/langgraph/concepts/low_level/#start-node) that sends user input to the graph, to indicate where to start our graph.\n",
    " \n",
    "The [`END` Node](https://langchain-ai.github.io/langgraph/concepts/low_level/#end-node) is a special node that represents a terminal node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END\n",
    "\n",
    "graph_builder.add_edge(START, \"retrieve_documents\")\n",
    "graph_builder.add_edge(\"retrieve_documents\", \"generate_response\")\n",
    "graph_builder.add_edge(\"generate_response\", END)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we [compile our graph](https://langchain-ai.github.io/langgraph/concepts/low_level/#compiling-your-graph) to perform a few basic checks on the graph structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import visualize_graph\n",
    "simple_rag_graph = graph_builder.compile()\n",
    "\n",
    "visualize_graph(simple_rag_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running our Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our graph is defined, let's invoke it!\n",
    "\n",
    "The compiled graph implements the [runnable](https://python.langchain.com/v0.1/docs/expression_language/interface/) protocol. This provides a standard way to execute LangChain components. `invoke` is one of the standard methods in this interface.\n",
    "\n",
    "The input is a dictionary `{\"question\": \"What is MongoDB used for?\"}`, which sets the initial value for our graph's state dictionary. Note that we didn't need to pass in all of the keys of our dictionary.\n",
    "\n",
    "Our graph executes as follows:\n",
    "1. When `invoke` is called, the graph starts execution from the `START` node.\n",
    "2. It progresses to `retrieve_documents` and invokes our retriever on the `question` defined in our State. It then writes the retrieved `documents` to State.\n",
    "3. It progresses to `generate_response` and makes an LLM call to generate an answer, using our retrieved `documents`.\n",
    "4. Finally, it progresses to the `END` node.\n",
    "\n",
    "Each node function receives the current state and returns a new value, which overrides the graph state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are some best practices for data backups in MongoDB?\"\n",
    "simple_rag_graph.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats on running your first LangGraph application! `invoke` runs the entire graph synchronously. This waits for each step to complete before moving to the next. It returns the final state of the graph after all nodes have executed, which is what we see above.\n",
    "\n",
    "Let's take a look in LangSmith!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Control Flow with Conditional Edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Corrective RAG](./images/conditional_rag.png)\n",
    "\n",
    "In this section, we're going to add a few techniques that can improve our RAG workflow. Specifically, we'll introduce\n",
    "- Document Grading: Are the documents fetched by the retriever actually relevant to the user's question?\n",
    "\n",
    "We're also going to add some constraints to the inputs and outputs of our application for the best user experience.\n",
    "\n",
    "By the end of this section, we'll have a more complex corrective RAG workflow! Then, we'll hop into LangSmith and walk through how we can evaluate that our application is actually improving as we add new techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some LLMs provide support for Structured Outputs, which provides a typing guarantee for the output schema of the LLM's response. Here, we can use BaseModel from pydantic to define a specific return type. The provided description helps the LLM generate the value for the field.\n",
    "\n",
    "We can hook this up to our previously defined `llm` using `with_structured_output`. Now, when we invoke our `grade_documents_llm`, we can expect the returned object to contain the expected field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    is_relevant: bool = Field(\n",
    "        description=\"The document is relevant to the question, true or false\"\n",
    "    )\n",
    "\n",
    "grade_documents_llm = llm.with_structured_output(GradeDocuments)\n",
    "grade_documents_system_prompt = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score true or false to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_documents_prompt = \"Here is the retrieved document: \\n\\n {document} \\n\\n Here is the user question: \\n\\n {question}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "    print(\"---GRADE DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        grade_documents_prompt_formatted = grade_documents_prompt.format(document=d.page_content, question=question)\n",
    "        score = grade_documents_llm.invoke(\n",
    "            [SystemMessage(content=grade_documents_system_prompt)] + [HumanMessage(content=grade_documents_prompt_formatted)]\n",
    "        )\n",
    "        grade = score.is_relevant\n",
    "        if grade:\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that at least some documents are relevant if we are going to respond to the user! To do this, we need to add a conditional edge. Once we add this conditional edge, we will define our graph again with our new node and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, END---\"\n",
    "        )\n",
    "        return \"none relevant\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"some relevant\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put our graph together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph import START, END\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"retrieve_documents\", retrieve_documents)\n",
    "graph_builder.add_node(\"generate_response\", generate_response)\n",
    "graph_builder.add_node(\"grade_documents\", grade_documents)    # new node!\n",
    "graph_builder.add_edge(START, \"retrieve_documents\")\n",
    "graph_builder.add_edge(\"retrieve_documents\", \"grade_documents\")    # edited edge\n",
    "graph_builder.add_conditional_edges(    # new conditional edge\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"some relevant\": \"generate_response\",\n",
    "        \"none relevant\": END\n",
    "    })\n",
    "graph_builder.add_edge(\"generate_response\", END)\n",
    "\n",
    "document_grading_graph = graph_builder.compile()\n",
    "visualize_graph(document_grading_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to invoke our graph again, this time with a question about something totally irrelevant, like pokemon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is your favorite pokemon?\"\n",
    "document_grading_graph.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Memory and Human-in-the-Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Term Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In every example so far, [state has been transient](https://github.com/langchain-ai/langgraph/discussions/352#discussioncomment-9291220) to a single graph execution. If we invoke our graph for a second time, we are starting with a fresh state. This limits our ability to have multi-turn conversations with interruptions. \n",
    "\n",
    "We can use [persistence](https://langchain-ai.github.io/langgraph/how-tos/persistence/) to address this! \n",
    " \n",
    "LangGraph can use a checkpointer to automatically save the graph state after each step. This built-in persistence layer gives us memory, allowing LangGraph to pick up from the last state update. \n",
    "\n",
    "Before we set up memory in our application, let's edit our State and Nodes so that instead of acting a single \"question\", we instead act on a list of \"questions and answers\".\n",
    "\n",
    "We'll call our list \"messages\". These existing messages will all be used for our retrieval step. And at the end of our flow when our LLM responds, we will add the latest question and answer to our \"messages\" history. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![breakpoints.jpg](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbae7985b747dfed67775d_breakpoints1.png)\n",
    "\n",
    "In this section, we'll talk about the different types of memory in LangGraph, and how we can use them to enable HIL workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import get_buffer_string\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "from langgraph.managed.is_last_step import RemainingSteps\n",
    "from typing import List\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    messages: Annotated[List[AnyMessage], add_messages]     # We now track a list of messages\n",
    "    documents: List[Document]\n",
    "    remaining_steps: RemainingSteps  # This is a special field that is used to track the remaining steps in the graph\n",
    "    # We removed generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's edit our existing Nodes to use `messages` in addition to `question`, specifically for grading document relevance, and generating a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_documents_system_prompt = \"\"\"You are a grader assessing relevance of a retrieved document to a conversation between a user and an AI assistant, and user's latest question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, definitely grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals that are not relevant at all. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_documents_prompt = \"Here is the retrieved document: \\n\\n {document} \\n\\n Here is the conversation so far: \\n\\n {conversation} \\n\\n Here is the user question: \\n\\n {question}\"\n",
    "def grade_documents(state):\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    conversation = get_buffer_string(state[\"messages\"])\n",
    "\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        grade_documents_prompt_formatted = grade_documents_prompt.format(document=d.page_content, question=question, conversation=conversation)\n",
    "        score = grade_documents_llm.invoke(\n",
    "            [SystemMessage(content=grade_documents_system_prompt)] + [HumanMessage(content=grade_documents_prompt_formatted)]\n",
    "        )\n",
    "        grade = score.is_relevant\n",
    "        if grade:\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT_WITH_CHAT_HISTORY = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "The pre-existing conversation may provide important context to the question.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Existing Conversation:\n",
    "{conversation}\n",
    "\n",
    "Latest Question:\n",
    "{question}\n",
    "\n",
    "Additional Context from Documents:\n",
    "{context} \n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "def generate_response(state: State):\n",
    "    print(\"---GENERATE RESPONSE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    conversation = get_buffer_string(state[\"messages\"])\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    \n",
    "    # RAG generation\n",
    "    rag_prompt_formatted = RAG_PROMPT_WITH_CHAT_HISTORY.format(context=formatted_docs, conversation=conversation, question=question)\n",
    "    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "    return {\n",
    "        \"messages\": [HumanMessage(content=question), generation],   # Add generation to our messages_list\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, now let's define our graph and add some local memory!\n",
    "\n",
    "One of the easiest to work with is `MemorySaver`, an in-memory key-value store for Graph state.\n",
    "\n",
    "All we need to do is compile the graph with a checkpointer, and our graph has memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our graph\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"retrieve_documents\", retrieve_documents)\n",
    "graph_builder.add_node(\"generate_response\", generate_response)\n",
    "graph_builder.add_node(\"grade_documents\", grade_documents)\n",
    "\n",
    "graph_builder.add_edge(START, \"retrieve_documents\")\n",
    "graph_builder.add_edge(\"retrieve_documents\", \"grade_documents\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"some relevant\": \"generate_response\",\n",
    "        \"none relevant\": END\n",
    "    })\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory = MemorySaver()\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "visualize_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MongoDB Checkpointer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use `MongoDBSaver` to persist the state of our graph in the MongoDB database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Checkpoint Thread](./images/checkpoint_thread.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.mongodb import MongoDBSaver\n",
    "\n",
    "# Initialize a MongoDB checkpointer\n",
    "mongodb_saver = MongoDBSaver(mongodb_client)\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=mongodb_saver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use memory, we need to specify a `thread_id`.\n",
    "\n",
    "This `thread_id` will store our collection of graph states.\n",
    "\n",
    "* The checkpointer write the state at every step of the graph\n",
    "* These checkpoints are saved in a thread \n",
    "* We can access that thread in the future using the `thread_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "thread_id = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "question = \"What are some best practices for data backups in MongoDB?\"\n",
    "response = graph.invoke({\"question\": question}, config)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask a follow-up with the same thread_id!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "question = \"What data format do you support?\"\n",
    "response = graph.invoke({\"question\": question}, config)\n",
    "for m in response[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human-in-the-Loop and \"Command\" module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HIL](./images/human_in_the_loop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's talk about the motivations for human-in-the-loop:\n",
    "\n",
    "1. **Approval** - We can interrupt our agent, surface state to a user, and allow the user to accept an action\n",
    "2. **Review and Edit** - You can view the state and edit it if necessary\n",
    "\n",
    "LangGraph offers several ways to get or update agent state to support various human-in-the-loop workflows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we'll focus on `interrupt()`\n",
    "\n",
    "When building human-in-the-loop into Python programs, one common way to do this is with the input function. With this, your program pauses, a text box pops up in your terminal, and whatever you type is then used as the response to that function. You use it like the below:\n",
    "\n",
    "`response = input(\"Your question here\")`\n",
    "\n",
    "We’ve tried to emulate this developer experience by adding a new function to LangGraph: interrupt. You can use this in much the same way as input:\n",
    "\n",
    "`response = interrupt(\"Your question here\")`\n",
    "\n",
    "This is designed to work in production settings. When you do this, it will pause execution of the graph, mark the thread you are running as interrupted, and put whatever you passed as an input to interrupt into the persistence layer. This way, you can check the thread status, see that it’s interrupted, check the message, and then based on that invoke the graph again (in a special way) to pass your response back in:\n",
    "\n",
    "`graph.invoke(Command(resume=\"Your response here\"), thread)`\n",
    "\n",
    "Note that it doesn’t function exactly the same as input (it reruns any work in that node done before this is called, but no previous nodes). This ensures interrupted threads don’t take up any resources (beyond storage space), and can be resumed many months later, on a different machine, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's add an interrupt step before we generate a response. We can use this opportunity view our state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import interrupt, Command\n",
    "\n",
    "def generate_response(state: State):\n",
    "    # We interrupt the graph, and ask the user for some additional context\n",
    "    additional_context = interrupt(\"Do you have anything else to add that you think is relevant?\")\n",
    "    print(\"---GENERATE RESPONSE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    # For simplicity, we'll just append the additional context to the conversation history\n",
    "    conversation = get_buffer_string(state[\"messages\"]) + additional_context\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    \n",
    "    rag_prompt_formatted = RAG_PROMPT_WITH_CHAT_HISTORY.format(context=formatted_docs, conversation=conversation, question=question)\n",
    "    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "    return {\n",
    "        \"messages\": [HumanMessage(content=question), generation],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our graph\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"retrieve_documents\", retrieve_documents)\n",
    "graph_builder.add_node(\"generate_response\", generate_response)\n",
    "graph_builder.add_node(\"grade_documents\", grade_documents)\n",
    "\n",
    "graph_builder.add_edge(START, \"retrieve_documents\")\n",
    "graph_builder.add_edge(\"retrieve_documents\", \"grade_documents\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"some relevant\": \"generate_response\",\n",
    "        \"none relevant\": END\n",
    "    })\n",
    "\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=mongodb_saver)\n",
    "# visualize_graph(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_id_3 = str(uuid.uuid4())\n",
    "config = {\"configurable\": {\"thread_id\": thread_id_3}}\n",
    "question = \"What are some best practices for data backups in MongoDB?\"\n",
    "graph.invoke({\"question\": question}, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Our graph has been interrupted! \n",
    "\n",
    "We can get the state and look at the next node to call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = graph.get_state(config)\n",
    "state.next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll introduce a nice trick. In order to resume the graph's execution, we can invoke the graph with an input `Command`.\n",
    "\n",
    "`Command` is a special type that when returned from a node specifies not only the update to the state (as usual) but also which node to go to next. This allows nodes to more directly control which nodes are executed after-the-fact. We can use it to resume the graph's execution after an interrupt!\n",
    "\n",
    "`graph.invoke(Command(resume=\"Your response here\"), thread)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke(Command(resume=\"I am using MongoDB Atlas\"), config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We quickly added a human in the loop to our graph using `interrupt()` and `Command`!\n",
    "\n",
    "`Command` can be used, as a replacement of conditional edges, inside nodes as well!\n",
    "Let's incorporate the conditional edge logic inside the `grade_documents` node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_documents(state: State) -> Command[Literal[\"generate_response\", \"__end__\"]]: # the return type is needed to visualize the connections in the graph\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    conversation = get_buffer_string(state[\"messages\"])\n",
    "\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        grade_documents_prompt_formatted = grade_documents_prompt.format(document=d.page_content, question=question, conversation=conversation)\n",
    "        score = grade_documents_llm.invoke(\n",
    "            [SystemMessage(content=grade_documents_system_prompt)] + [HumanMessage(content=grade_documents_prompt_formatted)]\n",
    "        )\n",
    "        grade = score.is_relevant\n",
    "        if grade:\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    \n",
    "    # Add new Command logic to route to the next node\n",
    "    if len(filtered_docs) > 0:\n",
    "        return Command(\n",
    "            # state update\n",
    "            update={\"documents\": filtered_docs},\n",
    "            # control flow\n",
    "            goto=\"generate_response\"\n",
    "        )\n",
    "        \n",
    "    return Command(\n",
    "                # state update\n",
    "                update={\n",
    "                    \"documents\": filtered_docs,\n",
    "                    \"messages\": [HumanMessage(content=\"No relevant documents found\")]\n",
    "                },\n",
    "                # control flow\n",
    "                goto=END\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our graph\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"retrieve_documents\", retrieve_documents)\n",
    "graph_builder.add_node(\"generate_response\", generate_response)\n",
    "graph_builder.add_node(\"grade_documents\", grade_documents)\n",
    "\n",
    "graph_builder.add_edge(START, \"retrieve_documents\")\n",
    "graph_builder.add_edge(\"retrieve_documents\", \"grade_documents\")\n",
    "# We removed the conditional edge\n",
    "\n",
    "\n",
    "graph = graph_builder.compile(\n",
    "    name=\"mongodb_rag_pipeline\",\n",
    "    checkpointer=mongodb_saver)\n",
    "visualize_graph(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_id_4 = str(uuid.uuid4())\n",
    "config = {\"configurable\": {\"thread_id\": thread_id_4}}\n",
    "question = \"What are some best practices for data backups in MongoDB?\"\n",
    "graph.invoke({\"question\": question}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke(Command(resume=\"Nothing else to add.\"), config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Building ReAct Agent using LangGraph Pre-built"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangGraph offers pre-built libraries for common architectures, allowing us to quickly create architectures like ReAct or multi-agent architacture. A full list of pre-built libraries can be found here: https://langchain-ai.github.io/langgraph/prebuilt/#available-libraries \n",
    "\n",
    "In the last workflow, we have seen how we can build a ReAct agent from scratch. Now, we will show how we can leverage the LangGraph pre-built libraries to achieve similar results. \n",
    "\n",
    "![react_2](./images/server_subagent.png)\n",
    "\n",
    "Our **MongoDB server subagent** is responsible for all customer queries related to the invoices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining tools and prompt\n",
    "Similarly, let's first define a set of tools and our agent prompt below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_mongodb_server_info() -> dict:\n",
    "    \"\"\"\n",
    "    Retrieve metadata about the connected MongoDB server instance.\n",
    "\n",
    "    This includes version, storage engines, JavaScript engine, and system-level information.\n",
    "    Useful for diagnostics or understanding server capabilities.\n",
    "\n",
    "    Returns:\n",
    "        dict: MongoDB server information such as version, build, and supported features.\n",
    "    \"\"\"\n",
    "    return mongodb_client.server_info()\n",
    "\n",
    "\n",
    "@tool\n",
    "def list_all_mongodb_databases() -> list[str]:\n",
    "    \"\"\"\n",
    "    List all available databases in the connected MongoDB server.\n",
    "\n",
    "    Use this tool to discover valid database names that can then be passed to other tools\n",
    "    such as `list_collections_in_database`.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: Names of all accessible MongoDB databases.\n",
    "    \"\"\"\n",
    "    return mongodb_client.list_database_names()\n",
    "\n",
    "\n",
    "@tool\n",
    "def list_collections_in_database(database_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Retrieve all collections and database-level statistics for a specified MongoDB database.\n",
    "\n",
    "    This tool combines collection metadata and stats into one response. Use it after calling\n",
    "    `list_all_mongodb_databases` to ensure the database name is valid.\n",
    "\n",
    "    Args:\n",
    "        database_name (str): The name of the MongoDB database to inspect.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "            - 'database': The name of the database.\n",
    "            - 'collections': A list of collection names in the database.\n",
    "            - 'stats': A dictionary with metrics like object count, storage size, index size, etc.\n",
    "    \"\"\"\n",
    "    db = mongodb_client[database_name]\n",
    "    collections = db.list_collection_names()\n",
    "    stats = db.command(\"dbstats\")\n",
    "    return {\n",
    "        \"database\": database_name,\n",
    "        \"collections\": collections,\n",
    "        \"stats\": stats\n",
    "    }\n",
    "\n",
    "mongodb_tools = [\n",
    "    get_mongodb_server_info,\n",
    "    list_all_mongodb_databases,\n",
    "    list_collections_in_database\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongodb_subagent_prompt = \"\"\"\n",
    "    You are a subagent among a team of assistants. You are specialized in retrieving and interpreting MongoDB metadata and structural information. You are routed for questions specifically related to MongoDB server info, database exploration, and collection listings — only respond to those.\n",
    "\n",
    "    You have access to three tools. These tools enable you to inspect the structure and capabilities of the connected MongoDB instance. Here are the tools:\n",
    "    - get_mongodb_server_info: This tool retrieves server-level information such as version, storage engines, and system details.\n",
    "    - list_all_mongodb_databases: This tool lists all available databases in the MongoDB instance.\n",
    "    - list_collections_in_database: This tool retrieves both the list of collections and database statistics for a specified database.\n",
    "\n",
    "    If the information is not available or an error occurs, inform the user clearly and ask if they would like to try a different query or database.\n",
    "\n",
    "    CORE RESPONSIBILITIES:\n",
    "    - Retrieve and summarize MongoDB server information for diagnostic or overview purposes\n",
    "    - Help users explore available databases and their internal structure (collections and stats)\n",
    "    - Combine data from multiple sources to provide a clear view of how MongoDB is organized\n",
    "    - Always maintain a professional, informative, and helpful tone\n",
    "    - You always need to provide a summary of the information you have retrieved.\n",
    "\n",
    "    You may have additional context that you should use to help answer the user's query. It will be provided to you below:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the pre-built library\n",
    "Now, let's put them together by using the pre-built ReAct agent library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Define the subagent\n",
    "mongodb_server_subagent = create_react_agent(\n",
    "    llm,\n",
    "    tools=mongodb_tools,\n",
    "    name=\"mongodb_server_subagent\",\n",
    "    prompt=mongodb_subagent_prompt,\n",
    "    state_schema=State,\n",
    "    checkpointer=mongodb_saver,\n",
    ")\n",
    "\n",
    "# Visualize the graph\n",
    "visualize_graph(mongodb_server_subagent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing!\n",
    "Let's try our new agent out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_id = uuid.uuid4()\n",
    "question = \"List me the available databases and the collections for the mongodb devday\"\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "result = mongodb_server_subagent.invoke({\"messages\": [HumanMessage(content=question)]}, config=config)\n",
    "for message in result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Building a Multi-Agent Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have two sub-agents that have different capabilities. How do we make sure customer tasks are appropriately routed between them? \n",
    "\n",
    "This is where the supervisor oversees the workflow, invoking appropriate subagents for relevant inquiries. \n",
    "\n",
    "\n",
    "A **multi-agent architecture** offers several key benefits:\n",
    "- Specialization & Modularity – Each sub-agent is optimized for a specific task, improving system accuracy \n",
    "- Flexibility – Agents can be quickly added, removed, or modified without affecting the entire system\n",
    "\n",
    "![supervisor](./images/multi_agent.png)\n",
    "\n",
    "We will show how we can utilize the pre-built supervisor to quickly create the multi-agent architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will create a set of instructions for our supervisor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor_prompt = \"\"\"\n",
    "You are an expert assistant tasked with helping users understand and troubleshoot MongoDB systems. \n",
    "Your job is to coordinate a team of specialized subagents to ensure that user queries are handled accurately and efficiently. \n",
    "You serve as the supervisor and planner for this multi-agent system, selecting the right subagent for each part of a user's request.\n",
    "\n",
    "You have access to the following two subagents:\n",
    "\n",
    "1. mongodb_server_subagent: This subagent specializes in retrieving live metadata from the MongoDB server. \n",
    "It can access information such as server version, available databases, collection names, and statistics about storage or object sizes. \n",
    "Use this subagent when the user asks about the structure, organization, or status of the MongoDB server or databases.\n",
    "\n",
    "2. mongodb_rag_pipeline: This subagent is powered by a retrieval-augmented generation (RAG) pipeline. \n",
    "It specializes in answering questions about MongoDB based on official documentation. \n",
    "Use this subagent when the user asks conceptual questions (e.g., \"How does indexing work?\", \"What is the difference between sharding and replication?\", etc.)\n",
    "\n",
    "Your job is to read the user's message and decide which subagent should be called next.\n",
    "Some queries may require multiple steps involving both subagents — plan accordingly and route to the most relevant subagent for the current step.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_supervisor import create_supervisor\n",
    "\n",
    "mongodb_rag_pipeline = graph\n",
    "# Create supervisor workflow\n",
    "supervisor_prebuilt_workflow = create_supervisor(\n",
    "    agents=[mongodb_rag_pipeline, mongodb_server_subagent],\n",
    "    output_mode=\"last_message\", # alternative is full_history\n",
    "    model=llm,\n",
    "    prompt=(supervisor_prompt), \n",
    "    state_schema=State\n",
    ")\n",
    "\n",
    "supervisor_agent = supervisor_prebuilt_workflow.compile(name=\"mongodb_supervisor_agent\", checkpointer=mongodb_saver)\n",
    "\n",
    "# Visualize the graph\n",
    "visualize_graph(supervisor_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_id = uuid.uuid4()\n",
    "question = \"What are some best practices for data backups in MongoDB?\"\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "result = supervisor_agent.invoke({\n",
    "    \"messages\": [HumanMessage(content=question)],\n",
    "    \"question\": question\n",
    "}, config=config)\n",
    "for message in result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = supervisor_agent.get_state(config)\n",
    "state.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = supervisor_agent.invoke(Command(resume=\"Nothing else to add.\"), config)\n",
    "for message in result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thread_id = uuid.uuid4()\n",
    "question = \"List me the available databases and the collections for the mongodb devday\"\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "result = supervisor_agent.invoke({\n",
    "    \"messages\": [HumanMessage(content=question)]\n",
    "}, config=config)\n",
    "for message in result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
